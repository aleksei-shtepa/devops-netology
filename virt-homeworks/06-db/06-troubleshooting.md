# Домашнее задание к занятию "6.6. Troubleshooting"

## Задача 1

> Перед выполнением задания ознакомьтесь с документацией по [администрированию MongoDB](https://docs.mongodb.com/manual/administration/).
>
> Пользователь (разработчик) написал в канал поддержки, что у него уже 3 минуты происходит CRUD операция в MongoDB и её нужно прервать.
>
> Вы как инженер поддержки решили произвести данную операцию:
>
> - напишите список операций, которые вы будете производить для остановки запроса пользователя
> - предложите вариант решения проблемы с долгими (зависающими) запросами в MongoDB

Из всех обрабатываемых на данный момент операций необходимо выделить те, что длятся больше трёх минут (180 секунд). Для этого подойдут команды `db.currentOp()` или `db.adminCommand()`:

```mongodb
db.currentOp(
   {
     "active" : true,
     "secs_running" : { "$gt" : 180 }
   }
)
```

```mongodb
db.adminCommand(
   {
     currentOp: true,
     "active" : true,
     "secs_running" : { "$gt" : 180 }
   }
)
```

В результате выполнения команд будет получен документ, поле `opid` в котором определяет идентификатор операции.

Идентификатор операции используется в команде `db.killOp()` для прерывания выполняемых операций.

Долгие ("подвисающие") операции - это негативное явление. Каждая такая операция должна быть исследована и оптимизирована. Выявить "подвисающие" операции может профилирование.

```mongodb
db.setProfilingLevel(1,200)
```

Данная команда запустит процесс, регистрирующий операции длительностью более 200 миллисекунд. Собранные профилировщиком данные сохраняются в коллекции `db.system.profile`.

## Задача 2

> Перед выполнением задания познакомьтесь с документацией по [Redis latency troobleshooting](https://redis.io/topics/latency).
>
>Вы запустили инстанс Redis для использования совместно с сервисом, который использует механизм TTL.
>
>Причем отношение количества записанных key-value значений к количеству истёкших значений есть величина постоянная и увеличивается пропорционально количеству реплик сервиса.
>
> При масштабировании сервиса до N реплик вы увидели, что:
>
> - сначала рост отношения записанных значений к истекшим
> - Redis блокирует операции записи
>
> Как вы думаете, в чем может быть проблема?

Судя по описанию проблема в большом количестве записей с истекшим сроком жизни (Time To Live, TTL).

**Redis** построен по однопоточной схеме и в связи с этим обладает радом особенностей.

Проверка окончания срока жизни записи производится периодически, по умолчанию каждые 100 миллисекунд. После проверки удаляется определённое количество истекших записей, по умолчанию 20. Если количество истекших записей превышает 25% от общего количетва записей, то цикл повторяется до тех пор, пока не будет достигнуто значение в 25%.

Документация на **Redis** сообщает, что при очистке истекших записей могут блокироваться операции добавления TTL-записей.

## Задача 3

> Перед выполнением задания познакомьтесь с документацией по [Common Mysql errors](https://dev.mysql.com/doc/refman/8.0/en/common-errors.html).
>
> Вы подняли базу данных MySQL для использования в гис-системе. При росте количества записей, в таблицах базы, пользователи начали жаловаться на ошибки вида:
>
> ```python
> InterfaceError: (InterfaceError) 2013: Lost connection to MySQL server during query u'SELECT..... '
> ```
>
> Как вы думаете, почему это начало происходить и как локализовать проблему?
>
> Какие пути решения данной проблемы вы можете предложить?

Сообщение возникает при обрыве соединения по истечению времени (during query timeout). То есть для запроса подготовлен слишком большой ответ, который не удалось передать за отведённое время (по умолчанию 30 секунд).

Для устранения проблемы необходимо оптимизировать запрос, возможно разбив его не несколько более мелких. Либо увеличить допустимое для передачи данных время через переменную **net_read_timeout**.

## Задача 4


> Вы решили перевести гис-систему из задачи 3 на PostgreSQL, так как прочитали в документации, что эта СУБД работает с большим объемом данных лучше, чем MySQL.
>
> После запуска пользователи начали жаловаться, что СУБД время от времени становится недоступной. В dmesg вы видите, что:
>
> `postmaster invoked oom-killer`
>
> Как вы думаете, что происходит?
>
> Как бы вы решили данную проблему?

Если СУБД содержит большой объём данных и иногда попадает под нож **Out-Of-Memory Killer**, то это означает нехватку выделяемой ей оперативной памяти.

Механизм **Out-Of-Memory Killer** стремится сохранить работоспособность операционной системы в ущерб процессам потребляющим слишком большой объём памяти.

Можно отключить этот механизм или понизить уровень процесса СУБД в рейтинге "плохих" процессов. Это решение выглядит плохим так как снижает надёжность системы.

Правильным решением будет настроить саму СУБД PostgreSQL. С учётом внутренней архитектуры и вариантов настройки СУБД существует формула рассчёта потребляемой оперативной памяти:
`Actual max RAM = shared_buffers + (temp_buffers + work_mem) * max_connections`

- **shared_buffers** - параметр задаёт объём памяти, который будет использовать сервер баз данных для буферов в разделяемой памяти. Разработчики СУБД рекомендуют значение этоко параметра выставлять в 25% от доступной памяти.

- **temp_buffers** - параметр задаёт максимальный объём памяти, выделяемой для временных буферов в каждом сеансе.

- **work_mem** - параметр задаёт базовый максимальный объём памяти, который будет использоваться во внутренних операциях при обработке запросов (например, для сортировки или хеш-таблиц), прежде чем будут задействованы временные файлы на диске. В сложных запросах параллельно могут выполняться несколько операций сортировки или хеширования, и при этом примерно этот объём памяти может использоваться в каждой операции, прежде чем данные начнут вытесняться во временные файлы.

- **max_connections** - параметр определяет максимальное число одновременных подключений к серверу БД.

Необходимо ограничить число одновременных подключений к СУБД используя мультиплексирование через пул подключений, например, **PgBouncer**. А так же поэкспериментировать со значениями `shared_buffers`, `temp_buffers` и `work_mem`.

Следующим шагом будет исследование "тяжёлых" запросов и их оптимизация.
Если и это не поможет, то остаётся только добавлять оперативной памяти в вычислительную машину.
